LFS
Sprite LFS also uses the segment summary information
to distinguish live blocks from those that have been
overwritten or deleted. Once a block’s identity is known,
its liveness can be determined by checking the file’s inode
or indirect block to see if the appropriate block pointer still
refers to this block. If it does, then the block is live; if it
doesn’t, then the block is dead. Sprite LFS optimizes this
check slightly by keeping a version number in the inode
map entry for each file; the version number is incremented
whenever the file is deleted or truncated to length zero.
The version number combined with the inode number form
an unique identifier (uid) for the contents of the file. The
segment summary block records this uid for each block in
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Old log end New log end
Copy and Compact Block Key:
Previously deleted
New data block
Old data block
Threaded log
Old log end New log end
Figure 2 — Possible free space management solutions for log-structured file systems.
In a log-structured file system, free space for the log can be generated either by copying the old blocks or by threading the log around the
old blocks. The left side of the figure shows the threaded log approach where the log skips over the active blocks and overwrites blocks of
files that have been deleted or overwritten. Pointers between the blocks of the log are maintained so that the log can be followed during
crash recovery. The right side of the figure shows the copying scheme where log space is generated by reading the section of disk after the
end of the log and rewriting the active blocks of that section along with the new data into the newly generated space.
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
the segment; if the uid of a block does not match the uid
currently stored in the inode map when the segment is
cleaned, the block can be discarded immediately without
examining the file’s inode.
This approach to cleaning means that there is no
free-block list or bitmap in Sprite. In addition to saving
memory and disk space, the elimination of these data structures
also simplifies crash recovery. If these data structures
existed, additional code would be needed to log changes to
the structures and restore consistency after crashes.
3.4. Segment cleaning policies
Given the basic mechanism described above, four
policy issues must be addressed:
(1) When should the segment cleaner execute? Some
possible choices are for it to run continuously in
background at low priority, or only at night, or only
when disk space is nearly exhausted.
(2) How many segments should it clean at a time? Segment
cleaning offers an opportunity to reorganize
data on disk; the more segments cleaned at once, the
more opportunities to rearrange.
(3) Which segments should be cleaned? An obvious
choice is the ones that are most fragmented, but this
turns out not to be the best choice.
(4) How should the live blocks be grouped when they
are written out? One possibility is to try to enhance
the locality of future reads, for example by grouping
files in the same directory together into a single output
segment. Another possibility is to sort the blocks
by the time they were last modified and group blocks
of similar age together into new segments; we call
this approach age sort.
July 24, 1991 - 5 -
In our work so far we have not methodically
addressed the first two of the above policies. Sprite LFS
starts cleaning segments when the number of clean segments
drops below a threshold value (typically a few tens
of segments). It cleans a few tens of segments at a time
until the number of clean segments surpasses another threshold
value (typically 50-100 clean segments). The overall
performance of Sprite LFS does not seem to be very sensitive
to the exact choice of the threshold values. In contrast,
the third and fourth policy decisions are critically important:
in our experience they are the primary factors that
determine the performance of a log-structured file system.
The remainder of Section 3 discusses our analysis of which
segments to clean and how to group the live data.
We use a term called write cost to compare cleaning
policies. The write cost is the average amount of time the
disk is busy per byte of new data written, including all the
cleaning overheads. The write cost is expressed as a multiple
of the time that would be required if there were no
cleaning overhead and the data could be written at its full
bandwidth with no seek time or rotational latency. A write
cost of 1.0 is perfect: it would mean that new data could be
written at the full disk bandwidth and there is no cleaning
overhead. A write cost of 10 means that only one-tenth of
the disk’s maximum bandwidth is actually used for writing
new data; the rest of the disk time is spent in seeks, rotational
latency, or cleaning.
For a log-structured file system with large segments,
seeks and rotational latency are negligible both for writing
and for cleaning, so the write cost is the total number of
bytes moved to and from the disk divided by the number of
those bytes that represent new data. This cost is determined
by the utilization (the fraction of data still live) in
the segments that are cleaned. In the steady state, the
cleaner must generate one clean segment for every segment
of new data written. To do this, it reads N segments in
their entirety and writes out N*u segments of live data
(where u is the utilization of the segments and 0 = u < 1).
This creates N*(1-u) segments of contiguous free space for
new data. Thus
write cost =
new data written
total bytes read and written hhhhhhhhhhhhhhhhhhhhhhh
=
new data written
hhhhhhhhhhhhhhhhhhhhhhhhhhhhread segs + write live + write new
= (1) N*(1-u)
N + N*u + N*(1-u) hhhhhhhhhhhhhhhhh=
1-u
2 hhhh
In the above formula we made the conservative assumption
that a segment must be read in its entirety to recover the
live blocks; in practice it may be faster to read just the live
blocks, particularly if the utilization is very low (we
haven’t tried this in Sprite LFS). If a segment to be cleaned
has no live blocks (u = 0) then it need not be read at all and
the write cost is 1.0.
Figure 3 graphs the write cost as a function of u. For
reference, Unix FFS on small-file workloads utilizes at
most 5-10% of the disk bandwidth, for a write cost of
10-20 (see [11] and Figure 8 in Section 5.1 for specific
measurements). With logging, delayed writes, and disk
request sorting this can probably be improved to about 25%
of the bandwidth[12] or a write cost of 4. Figure 3 suggests
that the segments cleaned must have a utilization of less
than .8 in order for a log-structured file system to outperform
the current Unix FFS; the utilization must be less than
.5 to outperform an improved Unix FFS.
It is important to note that the utilization discussed
above is not the overall fraction of the disk containing live
data; it is just the fraction of live blocks in segments that
are cleaned. Variations in file usage will cause some segments
to be less utilized than others, and the cleaner can
choose the least utilized segments to clean; these will have
lower utilization than the overall average for the disk.
Even so, the performance of a log-structured file system
can be improved by reducing the overall utilization of
the disk space. With less of the disk in use the segments
that are cleaned will have fewer live blocks resulting in a
lower write cost. Log-structured file systems provide a
cost-performance tradeoff: if disk space is underutilized,
higher performance can be achieved but at a high cost per
usable byte; if disk capacity utilization is increased, storage
costs are reduced but so is performance. Such a tradeoff
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
Log-structured
FFS improved
FFS today
Write cost
0.0
2.0
4.0
6.0
8.0
10.0
12.0
14.0
0.0 0.2 0.4 0.6 0.8 1.0
Fraction alive in segment cleaned (u)
Figure 3 — Write cost as a function of u for small files.
In a log-structured file system, the write cost depends strongly on
the utilization of the segments that are cleaned. The more live
data in segments cleaned the more disk bandwidth that is needed
for cleaning and not available for writing new data. The figure
also shows two reference points: ‘‘FFS today’’, which represents
Unix FFS today, and ‘‘FFS improved’’, which is our estimate of
the best performance possible in an improved Unix FFS. Write
cost for Unix FFS is not sensitive to the amount of disk space in
use.
hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh
July 24, 1991 - 6 -
between performance and space utilization is not unique to
log-structured file systems. For example, Unix FFS only
allows 90% of the disk space to be occupied by files. The
remaining 10% is kept free to allow the space allocation
algorithm to operate efficiently.
The key to achieving high performance at low cost in
a log-structured file system is to force the disk into a bimodal
segment distribution where most of the segments are
nearly full, a few are empty or nearly empty, and the
cleaner can almost always work with the empty segments.
This allows a high overall disk capacity utilization yet provides
a low write cost. The following section describes
how we achieve such a bimodal distribution in Sprite LFS.
3.5. Simulation results
We built a simple file system simulator so that we
could analyze different cleaning policies under controlled
conditions. The simulator’s model does not reflect actual
file system usage patterns (its model is much harsher than
reality), but it helped us to understand the effects of random
access patterns and locality, both of which can be
exploited to reduce the cost of cleaning. The simulator
models a file system as a fixed number of 4-kbyte files,
with the number chosen to produce a particular overall disk
capacity utilization. At each step, the simulator overwrites
one of the files with new data, using one of two pseudorandom
access patterns:
Uniform Each file has equal likelihood of being
